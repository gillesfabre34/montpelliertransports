## ğŸš€ Real-Time Public Transport Data Platform (Montpellier)

An end-to-end real-time data platform for Montpellier public transport data, from GTFS-RT ingestion to analytics-ready datasets for BI dashboards.

---

## ğŸ¯ Objective

**Build a real-time data platform that:**

- **Ingests** public transport GTFS-RT data continuously  
- **Streams** events through **Kafka** (Kraft mode) 
- **Processes** them using **PySpark Structured Streaming**  
- **Stores** them in **PostgreSQL (Azure)**  
- **Models** analytics-ready datasets using **dbt**  
- **Orchestrates** pipelines with **Airflow**  
- **Exposes** a clean analytics layer for BI tools  

---

## ğŸŒ Data Source

**Real-time GTFS-RT feeds from the Montpellier public transport system.**

- **Includes:**
  - Vehicle positions  
  - Trip updates  
  - Service alerts  

- **Format:**
  - Protobuf (GTFS Realtime standard)  
  - Frequently updated (near real-time)  

---

## ğŸ—ï¸ High-Level Architecture

```text
GTFS-RT API (Montpellier)
        â†“
   Python Producer
        â†“
       Kafka
        â†“
PySpark Structured Streaming
        â†“
 PostgreSQL (Azure)
        â†“
        dbt
        â†“
   BI / Dashboard
```

**Orchestration**: Apache Airflow

---

## ğŸ”§ Tech Stack

### â˜ï¸ Cloud

- Microsoft Azure  
  - Azure Database for PostgreSQL â€“ Flexible Server (free tier for 12 months)

### ğŸ—„ Database

- PostgreSQL (local for development, Azure for cloud deployment)

### ğŸ“¡ Streaming Layer

- Apache Kafka (Docker or Azure VM)

**Example topics:**

- `vehicle_positions_raw`  
- `vehicle_positions_clean`  
- `trip_updates`  
- `alerts`  

### ğŸ”¥ Processing

- Apache Spark  
- PySpark Structured Streaming  

### ğŸ Programming Language

- Python (core language across the platform)

### ğŸ“Š Analytics Modeling

- dbt (running on PostgreSQL)

### ğŸ›« Orchestration

- Apache Airflow (Dockerized)

---

## ğŸ Python Usage Across the Platform

### 1ï¸âƒ£ Ingestion Layer (Python Producer)

Python service that:

- Polls the GTFS-RT API  
- Parses protobuf messages  
- Converts them to structured JSON  
- Publishes events to Kafka  

**Libraries:**

- `requests`  
- `gtfs-realtime-bindings`  
- `confluent-kafka`  
- `pandas` (for normalization / validation if needed)  

---

### 2ï¸âƒ£ Streaming Processing (PySpark)

Spark job written in Python that:

- Reads from Kafka topics  
- Parses structured JSON schemas  
- Handles late events  
- Applies window aggregations  
- Performs data cleaning and normalization  
- Writes structured results to PostgreSQL  

**Key concepts:**

- Structured Streaming  
- Event-time processing  
- Watermarking  
- Window functions  
- Aggregations  

---

### 3ï¸âƒ£ Pandas Usage

Pandas is used for:

- Initial data exploration  
- Transformation prototyping before Spark implementation  
- Unit testing transformations  
- Feature engineering  
- Optional ML experiments  

---

### 4ï¸âƒ£ Airflow (Orchestration)

Airflow DAGs written in Python handle:

- Ingestion scheduling  
- Spark job execution  
- dbt runs  
- Data quality checks  
- Alerting  

Airflow provides Python-based orchestration for the entire platform.

---

## ğŸ§± Data Architecture (Medallion Pattern)

### Bronze Layer

- Raw ingested data from Kafka  
- Minimal transformation  
- Append-only  
- Full history preserved  

### Silver Layer

- Cleaned and normalized data:  
  - Deduplication  
  - Timestamp standardization  
  - Schema validation  
  - Geospatial normalization  

### Gold Layer

- Business-ready tables, such as:  
  - Average delay per line  
  - Station congestion metrics  
  - Traffic activity by time window  
  - Peak usage detection  
  - Anomaly detection  
  - Optional forecasting features  

---

## ğŸ“Š Example Analytics Use Cases

- Most congested stations  
- Average delay per route  
- Hourly traffic activity  
- Peak usage detection  
- Real-time anomaly detection  
- Transport load evolution over time  
- Optional: prediction of station saturation  

---

## ğŸ“ Suggested Repository Structure

```text
project/
â”‚
â”œâ”€â”€ producer/
â”‚   â”œâ”€â”€ gtfs_producer.py
â”‚   â””â”€â”€ config.py
â”‚
â”œâ”€â”€ spark/
â”‚   â”œâ”€â”€ stream_processor.py
â”‚   â””â”€â”€ schemas.py
â”‚
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/
â”‚       â”œâ”€â”€ ingestion_dag.py
â”‚       â”œâ”€â”€ processing_dag.py
â”‚       â””â”€â”€ dbt_dag.py
â”‚
â”œâ”€â”€ dbt/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ bronze/
â”‚   â”‚   â”œâ”€â”€ silver/
â”‚   â”‚   â””â”€â”€ gold/
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Project Phases

### Phase 1 â€“ Local Setup

- Kafka (Docker)  
- Spark (Docker)  
- Airflow (Docker)  
- Local PostgreSQL  
- Working Python producer  

### Phase 2 â€“ Streaming Pipeline

- Reliable Spark streaming job  
- Schema management  
- Error handling  
- Writing to PostgreSQL  

### Phase 3 â€“ Cloud Deployment

- PostgreSQL on Azure  
- Secure connections  
- Deployment strategy  

### Phase 4 â€“ Analytics Layer

- dbt models (Bronze â†’ Silver â†’ Gold)  
- Tests  
- Documentation  
- Data quality checks  

### Phase 5 â€“ Advanced

- Great Expectations  
- CI/CD  
- Monitoring  
- ML forecasting  
- Infrastructure automation (Terraform optional)  

